{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "# explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "# now you can import normally from sklearn.impute\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.svm import LinearSVR, SVR, SVC,LinearSVC\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import normalize \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_original = pd.read_csv('train_features.csv')\n",
    "df_training_label = pd.read_csv('train_labels.csv')\n",
    "all_pids = [pid for pid in df_training_original['pid'].unique()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPatientData(trainingData, pids, patients=0, mode='pid'):\n",
    "    if mode == 'number':\n",
    "        pids = all_pids[:patients]\n",
    "    if len(pids) == 0:\n",
    "        return trainingData\n",
    "    #pids = np.array(pids).astype(np.float)\n",
    "    patients = [trainingData.iloc[idx] for idx in range(0, len(trainingData)) if trainingData['pid'][idx] in pids]    \n",
    "    #patientTrainingDataIndex = [trainingData.iloc[idx] for idx, col in enumerate(trainingData) if trainingData['pid'][idx] in pids]    \n",
    "    return pd.DataFrame(patients)\n",
    "\n",
    "def partitionData(trainingDataPids, trainingPartition=80):\n",
    "    validationPartition = 100 - trainingPartition\n",
    "    countTraining = int((trainingPartition/100)*len(trainingDataPids))\n",
    "    training = trainingDataPids[:countTraining]\n",
    "    validation = trainingDataPids[countTraining:]\n",
    "    print('')\n",
    "    print('Training size: ' + str(countTraining))\n",
    "    print('Validation size: ' + str(len(validation)))\n",
    "    return training, validation\n",
    "\n",
    "def populateData(X,Y):\n",
    "    Z = pd.merge(X, Y, on='pid')\n",
    "    YData = Z[Y.columns].iloc[:,1:]\n",
    "    XData = Z[X.columns].iloc[:,1:]\n",
    "    return XData, YData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in data:\n",
      "FiO2                3246.0\n",
      "BaseExcess          1267.0\n",
      "Creatinine           837.0\n",
      "Lactate              295.0\n",
      "Bilirubin_total      219.0\n",
      "RRate                101.0\n",
      "Magnesium             59.0\n",
      "Phosphate             59.0\n",
      "Bilirubin_direct      19.0\n",
      "Calcium               19.0\n",
      "TroponinI             11.0\n",
      "BUN                    9.0\n",
      "WBC                    8.0\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Oliver\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training size: 15196\n",
      "Validation size: 3799\n"
     ]
    }
   ],
   "source": [
    "df_training = df_training_original.copy()\n",
    "\n",
    "# See how many values are missing in which rows\n",
    "loss = df_training_original.isnull().any()\n",
    "lossRow = df_training[df_training_original == loss].sum()\n",
    "print('Number of missing values in data:')\n",
    "print(lossRow[lossRow > 0].sort_values(ascending=False))\n",
    "\n",
    "# Filter out certain rows with a lot of missing data\n",
    "lossColumns = list(lossRow[lossRow > 100].index)\n",
    "df_training = df_training.drop(columns=lossColumns)\n",
    "#df_training_label.drop(columns=lossColumns, axis=0, inplace=True)\n",
    "\n",
    "# Set all time data to be between 1-12\n",
    "X_ALL = pd.DataFrame(columns = df_training.columns)\n",
    "for pids in all_pids:\n",
    "    df_trainingTemp = df_training[df_training['pid'] == pids]\n",
    "    df_trainingTemp['Time'] = df_trainingTemp['Time'] - (df_trainingTemp['Time'].min() -1)\n",
    "    X_ALL = pd.concat([X_ALL, df_trainingTemp])\n",
    "    \n",
    "df_training = X_ALL.copy()\n",
    "# Partition data in training and validation\n",
    "trainingPIDS, validationPIDS = partitionData(all_pids)\n",
    "X_pid = getPatientData(df_training, trainingPIDS)\n",
    "X_pid_val = getPatientData(df_training, validationPIDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Magnesium', 'Phosphate', 'Bilirubin_direct', 'Calcium', 'TroponinI',\n",
      "       'BUN', 'WBC'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_data_loss = pd.DataFrame(index=df_training.count().index,columns=['Entries', 'MissingEntries', 'Percentage Missing'])\n",
    "#print(df_training_original.count())\n",
    "# See how many values are missing in which rows\n",
    "loss = df_training.isnull().any()\n",
    "lossRow = df_training[df_training == loss].sum()\n",
    "#print('Number of missing values in data:')\n",
    "#print(lossRow[lossRow > 0].sort_values(ascending=False))\n",
    "temp = lossRow[lossRow > 0].sort_values(ascending=False)\n",
    "print(temp.index)\n",
    "colNames = temp.index\n",
    "entries = pd.DataFrame(df_training.isnull().count(), columns=['Entries'])\n",
    "missingEntries = df_training.isnull().sum()\n",
    "percentage = missingEntries /df_training.isnull().count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Entries  MissingEntries  Percentage Missing\n",
      "pid                227940               0            0.000000\n",
      "Time               227940               0            0.000000\n",
      "Age                227940               0            0.000000\n",
      "Heartrate          227940           27812            0.122015\n",
      "ABPm               227940           32051            0.140612\n",
      "SpO2               227940           32748            0.143669\n",
      "ABPs               227940           36290            0.159209\n",
      "ABPd               227940           75522            0.331324\n",
      "Temp               227940          146825            0.644139\n",
      "Glucose            227940          180904            0.793647\n",
      "Potassium          227940          199547            0.875437\n",
      "Hct                227940          200643            0.880245\n",
      "pH                 227940          202894            0.890120\n",
      "Hgb                227940          205645            0.902189\n",
      "PaCO2              227940          206897            0.907682\n",
      "BUN                227940          207835            0.911797\n",
      "WBC                227940          208857            0.916281\n",
      "Platelets          227940          209905            0.920878\n",
      "Calcium            227940          210110            0.921778\n",
      "Magnesium          227940          210417            0.923125\n",
      "Chloride           227940          214023            0.938944\n",
      "SaO2               227940          214926            0.942906\n",
      "HCO3               227940          215381            0.944902\n",
      "Phosphate          227940          216350            0.949153\n",
      "PTT                227940          217641            0.954817\n",
      "EtCO2              227940          218157            0.957081\n",
      "AST                227940          222179            0.974726\n",
      "Alkalinephos       227940          222232            0.974958\n",
      "TroponinI          227940          224164            0.983434\n",
      "Fibrinogen         227940          225447            0.989063\n",
      "Bilirubin_direct   227940          227221            0.996846\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_data_total = df_data_loss.copy()\n",
    "df_data_total['Entries'] = entries\n",
    "df_data_total['MissingEntries'] = missingEntries\n",
    "df_data_total['Percentage Missing'] = percentage\n",
    "\n",
    "print(df_data_total.sort_values(by='Percentage Missing', ascending=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation\n",
    "#X_pid = df_training\n",
    "my_imputer = IterativeImputer()\n",
    "imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_pid))\n",
    "imputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_pid_val))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train_plus.columns = X_pid.columns\n",
    "imputed_X_valid_plus.columns = X_pid_val.columns\n",
    "\n",
    "print(imputed_X_valid_plus.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only take first X patients\n",
    "X_pid_train = getPatientData(imputed_X_train_plus, [], patients=1000, mode='number')\n",
    "Y_pid_train = getPatientData(df_training_label, [], patients=1000, mode='number')\n",
    "X_val = imputed_X_valid_plus\n",
    "Y_val = getPatientData(df_training_label, validationPIDS)\n",
    "\n",
    "# Make X and Y the same size, remove pid\n",
    "X_train_proc, Y_train_proc = populateData(X_pid_train, Y_pid_train)\n",
    "X_val_proc, Y_val_proc = populateData(X_val, Y_val)\n",
    "\n",
    "# Normalize Data\n",
    "norm_Xtrain = pd.DataFrame(normalize(X_train_proc, norm='max',axis=0))\n",
    "norm_Xtrain.columns = X_train_proc.columns\n",
    "norm_Xval = pd.DataFrame(normalize(X_val_proc, norm='max',axis=0))\n",
    "norm_Xval.columns = X_val_proc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Time   Age      EtCO2        PTT        BUN       Temp        Hgb  \\\n",
      "0   1.0  34.0  33.367981  38.804622  12.000000  36.000000   8.700000   \n",
      "1   2.0  34.0  33.608720  39.365101  23.133962  36.000000  10.641975   \n",
      "2   3.0  34.0  34.271618  39.565520  23.133622  36.000000  10.641970   \n",
      "3   4.0  34.0  34.554130  39.514079  23.133601  37.000000  10.642187   \n",
      "4   5.0  34.0  34.396151  39.170251  23.133716  36.854604  10.640923   \n",
      "\n",
      "        HCO3  Fibrinogen  Phosphate  ...   Calcium  Alkalinephos   SpO2  \\\n",
      "0  24.000000  215.585891   3.253653  ...  7.151633     96.077986  100.0   \n",
      "1  22.954709  282.503029   3.753947  ...  7.171375    103.833333  100.0   \n",
      "2  23.267328  277.311829   3.644657  ...  7.167266    102.677776  100.0   \n",
      "3  23.393672  287.110658   3.614850  ...  7.168602    101.700022  100.0   \n",
      "4  24.163537  248.346468   3.389598  ...  7.155465    100.148955  100.0   \n",
      "\n",
      "   Bilirubin_direct    Chloride        Hct  Heartrate  TroponinI   ABPs    pH  \n",
      "0          1.449792  114.000000  24.600000       94.0   2.141424  142.0  7.33  \n",
      "1          1.644639  106.237653  31.317483       99.0   9.852175  125.0  7.33  \n",
      "2          1.549101  106.233141  31.319374       92.0  10.911312  110.0  7.37  \n",
      "3          1.528988  106.232046  31.317831       88.0   9.588790  104.0  7.37  \n",
      "4          1.533990  106.235815  22.400000       81.0  10.139546  100.0  7.41  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_proc.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Oliver\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='sigmoid', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr = SVC(kernel='sigmoid', C=1.0)\n",
    "regr.fit(np.array(norm_Xtrain), np.array(Y_train_proc.iloc[:,0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = regr.predict(norm_Xval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(f)[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.265\n"
     ]
    }
   ],
   "source": [
    "print(mean_absolute_error(f, Y_val_proc.iloc[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Oliver\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3063, 26)\n"
     ]
    }
   ],
   "source": [
    "## Trying a different approach ##\n",
    "\n",
    "\n",
    "X_pid_train = pd.DataFrame(columns=['pid','Age', 'Temp1','Temp2','Temp3','Temp4','Temp5','Temp6',\n",
    "                              'HR1','HR2','HR3','HR4','HR5','HR6',\n",
    "                              'Abps1','Abps2','Abps3','Abps4','Abps5','Abps6',\n",
    "                              'pH1','pH2','pH3','pH4','pH5','pH6'])\n",
    "\n",
    "Y_pid_train = pd.DataFrame(columns = df_training_label.columns)\n",
    "for pid in all_pids:\n",
    "    uniqueData = df_training_original[df_training_original['pid']==pid]\n",
    "    yResult = df_training_label[df_training_label['pid'] == pid]\n",
    "    yResult['pid'] = yResult['pid'].astype(np.float)\n",
    "    Temp = uniqueData['Temp']\n",
    "    if Temp.isna().sum() > 3:\n",
    "        continue\n",
    "    HR = uniqueData['Heartrate']\n",
    "    Abps = uniqueData['ABPs']\n",
    "    pH  = uniqueData['pH']\n",
    "    DF = pd.DataFrame([[np.nan,np.nan, np.nan, np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,\n",
    "                       np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,\n",
    "                       np.nan,np.nan,np.nan,np.nan,np.nan]], columns=['pid','Age', 'Temp1','Temp2','Temp3','Temp4','Temp5','Temp6',\n",
    "                              'HR1','HR2','HR3','HR4','HR5','HR6',\n",
    "                              'Abps1','Abps2','Abps3','Abps4','Abps5','Abps6',\n",
    "                              'pH1','pH2','pH3','pH4','pH5','pH6'])\n",
    "\n",
    "    DF['pid'] = uniqueData['pid'].iloc[0]\n",
    "    DF['Age'] = uniqueData['Age'].iloc[0]\n",
    "    for i in range(0,6):\n",
    "        DF['Temp' + str(i+1)] = Temp.iloc[2*i]\n",
    "        DF['HR' + str(i+1)] = HR.iloc[2*i]\n",
    "        DF['Abps' + str(i+1)] = Abps.iloc[2*i]\n",
    "        DF['pH' + str(i+1)] = pH.iloc[2*i]\n",
    "\n",
    "\n",
    "    X_pid_train = pd.concat([X_pid_train, DF])\n",
    "    Y_pid_train = pd.concat([Y_pid_train, yResult])\n",
    "\n",
    "print(X_pid_train.shape)\n",
    "#print(Y_pid_train)\n",
    "#regr.fit(np.array(L), np.array(Y_train_proc.iloc[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3063, 26)\n"
     ]
    }
   ],
   "source": [
    "my_imputer2 = IterativeImputer()\n",
    "imputed_X_train_plus2 = pd.DataFrame(my_imputer2.fit_transform(X_pid_train))\n",
    "imputed_X_train_plus2.columns = X_pid_train.columns\n",
    "\n",
    "print(imputed_X_train_plus2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training size: 2450\n",
      "Validation size: 613\n"
     ]
    }
   ],
   "source": [
    "imputed_X_train_plus2 = imputed_X_train_plus2.reset_index(drop=True)\n",
    "Y_pid_train = Y_pid_train.reset_index(drop=True)\n",
    "\n",
    "trainingPIDS, validationPIDS = partitionData([pid for pid in imputed_X_train_plus2['pid'].unique()])\n",
    "X_pid = getPatientData(imputed_X_train_plus2, trainingPIDS)\n",
    "Y_pid = getPatientData(Y_pid_train, trainingPIDS)\n",
    "X_pid_val = getPatientData(imputed_X_train_plus2, validationPIDS)\n",
    "Y_pid_val = getPatientData(Y_pid_train, validationPIDS)\n",
    "\n",
    "# Drop pid\n",
    "X_pid = X_pid.iloc[:,1:]\n",
    "Y_pid = Y_pid.iloc[:,1:]\n",
    "X_pid_val = X_pid_val.iloc[:,1:]\n",
    "Y_pid_val = Y_pid_val.iloc[:,1:]\n",
    "\n",
    "X_pid_train_norm = pd.DataFrame(normalize(X_pid,axis=0))\n",
    "X_pid_train_norm.columns = X_pid.columns\n",
    "X_pid_val_norm = pd.DataFrame(normalize(X_pid_val,axis=0))\n",
    "X_pid_val_norm.columns = X_pid_val.columns\n",
    "#print(norm_Xtrain.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr = LinearSVC(C=1)\n",
    "regr.fit(np.array(X_pid_train_norm), np.array(Y_pid.iloc[:,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "GT\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "f = regr.predict(X_pid_val_norm)\n",
    "print('Prediction')\n",
    "print(f)\n",
    "print('')\n",
    "print('GT')\n",
    "print(np.array(Y_pid_val.iloc[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09787928221859707\n"
     ]
    }
   ],
   "source": [
    "print(mean_absolute_error(f, np.array(Y_pid_val.iloc[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
