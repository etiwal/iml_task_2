{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "# explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "# now you can import normally from sklearn.impute\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.svm import LinearSVR, SVR, SVC,LinearSVC\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import normalize \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, RidgeClassifierCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "df_training_original = pd.read_csv('train_features.csv')\n",
    "df_training_label = pd.read_csv('train_labels.csv')\n",
    "all_pids = [pid for pid in df_training_original['pid'].unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPatientData(trainingData, pids, patients=0, mode='pid'):\n",
    "    if mode == 'number':\n",
    "        pids = all_pids[:patients]\n",
    "    if len(pids) == 0:\n",
    "        return trainingData\n",
    "    #pids = np.array(pids).astype(np.float)\n",
    "    patients = [trainingData.iloc[idx] for idx in range(0, len(trainingData)) if trainingData['pid'][idx] in pids]    \n",
    "    #patientTrainingDataIndex = [trainingData.iloc[idx] for idx, col in enumerate(trainingData) if trainingData['pid'][idx] in pids]    \n",
    "    return pd.DataFrame(patients)\n",
    "\n",
    "def partitionData(trainingDataPids, trainingPartition=80):\n",
    "    validationPartition = 100 - trainingPartition\n",
    "    countTraining = int((trainingPartition/100)*len(trainingDataPids))\n",
    "    training = trainingDataPids[:countTraining]\n",
    "    validation = trainingDataPids[countTraining:]\n",
    "    print('')\n",
    "    print('Training size: ' + str(countTraining))\n",
    "    print('Validation size: ' + str(len(validation)))\n",
    "    return training, validation\n",
    "\n",
    "def populateData(X,Y):\n",
    "    Z = pd.merge(X, Y, on='pid')\n",
    "    YData = Z[Y.columns].iloc[:,1:]\n",
    "    XData = Z[X.columns].iloc[:,1:]\n",
    "    return XData, YData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in data:\n",
      "Series([], dtype: float64)\n",
      "\n",
      "Training size: 15196\n",
      "Validation size: 3799\n"
     ]
    }
   ],
   "source": [
    "df_training = df_training_original.copy()\n",
    "\n",
    "# See how many values are missing in which rows\n",
    "loss = df_training_original.isnull().any()\n",
    "lossRow = df_training[df_training_original == loss].sum()\n",
    "print('Number of missing values in data:')\n",
    "print(lossRow[lossRow > 0].sort_values(ascending=False))\n",
    "\n",
    "# Filter out certain rows with a lot of missing data\n",
    "lossColumns = list(lossRow[lossRow > 100].index)\n",
    "df_training = df_training.drop(columns=lossColumns)\n",
    "\n",
    "# Set all time data to be between 1-12\n",
    "X_ALL = pd.DataFrame(columns = df_training.columns)\n",
    "if os.path.exists('x_time_adjusted.csv'):\n",
    "    X_ALL = pd.read_csv('x_time_adjusted.csv', index_col=False)\n",
    "    X_ALL = X_ALL.iloc[:,1:]\n",
    "else:\n",
    "    for pids in all_pids:\n",
    "        df_trainingTemp = df_training[df_training['pid'] == pids]\n",
    "        df_trainingTemp['Time'] = df_trainingTemp['Time'] - (df_trainingTemp['Time'].min() -1)\n",
    "        X_ALL = pd.concat([X_ALL, df_trainingTemp])\n",
    "    \n",
    "df_training = X_ALL.copy()\n",
    "# Partition data in training and validation\n",
    "trainingPIDS, validationPIDS = partitionData(all_pids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "## Divide data in half\n",
    "\n",
    "X_pid_train = pd.DataFrame(columns=['pid','Age', 'Temp1','Temp2','Temp3','Temp4','Temp5','Temp6',\n",
    "                              'HR1','HR2','HR3','HR4','HR5','HR6',\n",
    "                              'Abps1','Abps2','Abps3','Abps4','Abps5','Abps6',\n",
    "                              'ABPm1','ABPm2','ABPm3','ABPm4','ABPm5','ABPm6',\n",
    "                                    'SpO21','SpO22','SpO23','SpO24','SpO25','SpO26',\n",
    "                                    'ABPd1','ABPd2','ABPd3','ABPd4','ABPd5','ABPd6'\n",
    "                                   ])\n",
    "\n",
    "Y_pid_train = pd.DataFrame(columns = df_training_label.columns)\n",
    "for pid in all_pids:\n",
    "    uniqueData = df_training_original[df_training_original['pid']==pid]\n",
    "    yResult = df_training_label[df_training_label['pid'] == pid]\n",
    "    yResult['pid'] = yResult['pid'].astype(np.float)\n",
    "    Temp = uniqueData['Temp']\n",
    "    #if Temp.isna().sum() > 3:\n",
    "    #    continue\n",
    "    HR = uniqueData['Heartrate']\n",
    "    Abps = uniqueData['ABPs']\n",
    "    Abpm = uniqueData['ABPm']\n",
    "    Abpd = uniqueData['ABPd']\n",
    "    \n",
    "    spo2 = uniqueData['SpO2']\n",
    "    pH  = uniqueData['pH']\n",
    "    DF = pd.DataFrame([[np.nan,np.nan, np.nan, np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,\n",
    "                        np.nan,np.nan, np.nan, np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,\n",
    "                        np.nan,np.nan, np.nan, np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,\n",
    "                        np.nan,np.nan, np.nan, np.nan,np.nan,np.nan,np.nan,np.nan]],\n",
    "                      columns=['pid','Age', 'Temp1','Temp2','Temp3','Temp4','Temp5','Temp6',\n",
    "                              'HR1','HR2','HR3','HR4','HR5','HR6',\n",
    "                              'Abps1','Abps2','Abps3','Abps4','Abps5','Abps6',\n",
    "                              'ABPm1','ABPm2','ABPm3','ABPm4','ABPm5','ABPm6',\n",
    "                                    'SpO21','SpO22','SpO23','SpO24','SpO25','SpO26',\n",
    "                                    'ABPd1','ABPd2','ABPd3','ABPd4','ABPd5','ABPd6'\n",
    "                                   ])\n",
    "\n",
    "    DF['pid'] = uniqueData['pid'].iloc[0]\n",
    "    DF['Age'] = uniqueData['Age'].iloc[0]\n",
    "    for i in range(0,6):\n",
    "        DF['Temp' + str(i+1)] = Temp.iloc[2*i]\n",
    "        DF['HR' + str(i+1)] = HR.iloc[2*i]\n",
    "        DF['Abps' + str(i+1)] = Abps.iloc[2*i]\n",
    "        DF['ABPm' + str(i+1)] = Abpm.iloc[2*i]\n",
    "        DF['SpO2' + str(i+1)] = spo2.iloc[2*i]\n",
    "        DF['ABPd' + str(i+1)] = Abpd.iloc[2*i]\n",
    "\n",
    "#        DF['pH' + str(i+1)] = pH.iloc[2*i]\n",
    "\n",
    "    X_pid_train = pd.concat([X_pid_train, DF])\n",
    "    Y_pid_train = pd.concat([Y_pid_train, yResult])\n",
    "print(X_pid_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only take every third datapoint\n",
    "\n",
    "X_pid_train = pd.DataFrame(columns=['pid','Age', \n",
    "                                    'Temp1','Temp2','Temp3',\n",
    "                              'HR1','HR2','HR3',\n",
    "                              'Abps1','Abps2','Abps3',\n",
    "                              'ABPm1','ABPm2','ABPm3',\n",
    "                                    'SpO21','SpO22','SpO23',\n",
    "                                    'ABPd1','ABPd2','ABPd3'\n",
    "                                   ])\n",
    "\n",
    "Y_pid_train = pd.DataFrame(columns = df_training_label.columns)\n",
    "for pid in all_pids[:100000]:\n",
    "    uniqueData = df_training_original[df_training_original['pid']==pid]\n",
    "    yResult = df_training_label[df_training_label['pid'] == pid]\n",
    "    yResult['pid'] = yResult['pid'].astype(np.float)\n",
    "    Temp = uniqueData['Temp']\n",
    "    #if Temp.isna().sum() > 3:\n",
    "    #    continue\n",
    "    HR = uniqueData['Heartrate']\n",
    "    Abps = uniqueData['ABPs']\n",
    "    Abpm = uniqueData['ABPm']\n",
    "    Abpd = uniqueData['ABPd']\n",
    "    spo2 = uniqueData['SpO2']\n",
    "    #pH  = uniqueData['pH']\n",
    "    \n",
    "    DF = pd.DataFrame([[np.nan, np.nan, np.nan, np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,\n",
    "                        np.nan,np.nan, np.nan, np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan]],\n",
    "                      columns=['pid','Age', \n",
    "                              'Temp1','Temp2','Temp3',\n",
    "                              'HR1','HR2','HR3',\n",
    "                              'Abps1','Abps2','Abps3',\n",
    "                              'ABPm1','ABPm2','ABPm3',\n",
    "                              'SpO21','SpO22','SpO23',\n",
    "                              'ABPd1','ABPd2','ABPd3'])\n",
    "\n",
    "    DF['pid'] = uniqueData['pid'].iloc[0]\n",
    "    DF['Age'] = uniqueData['Age'].iloc[0]\n",
    "    for i, j in enumerate([0,5,11]):\n",
    "        DF['Temp' + str(i+1)] = Temp.iloc[j]\n",
    "        DF['HR' + str(i+1)] = HR.iloc[j]\n",
    "        DF['Abps' + str(i+1)] = Abps.iloc[j]\n",
    "        DF['ABPm' + str(i+1)] = Abpm.iloc[j]\n",
    "        DF['SpO2' + str(i+1)] = spo2.iloc[j]\n",
    "        DF['ABPd' + str(i+1)] = Abpd.iloc[j]\n",
    "\n",
    "#        DF['pH' + str(i+1)] = pH.iloc[2*i]\n",
    "    X_pid_train = pd.concat([X_pid_train, DF])\n",
    "    Y_pid_train = pd.concat([Y_pid_train, yResult])\n",
    "print(X_pid_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_pid_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_imputer = SimpleImputer(strategy='mean')\n",
    "imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_pid_train))\n",
    "imputed_X_train_plus.columns = X_pid_train.columns\n",
    "print(imputed_X_train_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "corr = all_pid_train[all_pid_train.columns[3:31]].corr()\n",
    "ax = sns.heatmap(\n",
    "    corr, \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sns.diverging_palette(20, 220, n=200),\n",
    "    square=True\n",
    ")\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=45,\n",
    "    horizontalalignment='right'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#regr = LinearSVC(C=1, fit_intercept=False, multi_class='cramer_singer')\n",
    "\n",
    "# Drop rows that contain 0 for all y targets (get rid of a lot of negatives)\n",
    "cols = df_training_label.columns\n",
    "indices = [df_training_label.iloc[x,:].name for x in range(0, len(df_training_label)) if not(df_training_label.iloc[x,1:12] == [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]).all()]\n",
    "df_training2 = df_training_label.loc[indices,:]\n",
    "X_filtered = imputed_X_train_plus.copy()\n",
    "X_norm = X_filtered.copy()\n",
    "X_norm = X_norm[X_norm['pid'].isin(list(df_training2['pid']))]\n",
    "y_pid_fit = Y_pid_train[Y_pid_train['pid'].isin(list(df_training2['pid']))]\n",
    "y_pid_fit = y_pid_fit.reset_index(drop=True)\n",
    "\n",
    "# Normalize the data\n",
    "X_norm.iloc[:,1:] = StandardScaler().fit_transform(X_norm.iloc[:,1:])\n",
    "X_norm = pd.DataFrame(X_norm)\n",
    "X_norm = X_norm.reset_index(drop=True)\n",
    "X_norm.columns = X_filtered.columns\n",
    "print(X_norm.shape)\n",
    "\n",
    "\n",
    "\n",
    "# Train MLPClassifier\n",
    "#regr =   KNeighborsClassifier(3)\n",
    "regr = MLPClassifier(alpha=1e-5,hidden_layer_sizes=(100,100), random_state=1, solver='sgd', max_iter=200)\n",
    "#regr = RidgeClassifierCV()\n",
    "#regr = RandomForestClassifier()\n",
    "\n",
    "regr.fit(np.array(X_norm.iloc[:11000,1:]), np.array(y_pid_fit.iloc[:11000,1:11]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = regr.predict_proba(X_norm.iloc[11000:,1:])\n",
    "\n",
    "f = pd.DataFrame(f)\n",
    "\n",
    "f.columns = Y_pid_train.columns[1:11]\n",
    "f['pid'] = X_norm.iloc[11000:,0].reset_index(drop=True)\n",
    "f['LABEL_Sepsis'] = np.nan\n",
    "f['LABEL_RRate'] = np.nan\n",
    "f['LABEL_ABPm'] = np.nan\n",
    "f['LABEL_SpO2'] = np.nan\n",
    "f['LABEL_Heartrate'] = np.nan\n",
    "f = f[Y_pid_train.columns]\n",
    "get_score(y_pid_fit.iloc[11000:,:], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mit daten in 3 geteilt 0.6663326701565544 (50,50)\n",
    "# mit 6 datenpunkten 0.6733103690366024 mit netz (100,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "TESTS = ['LABEL_BaseExcess', 'LABEL_Fibrinogen', 'LABEL_AST', 'LABEL_Alkalinephos', 'LABEL_Bilirubin_total',\n",
    "         'LABEL_Lactate', 'LABEL_TroponinI', 'LABEL_SaO2',\n",
    "         'LABEL_Bilirubin_direct', 'LABEL_EtCO2']\n",
    "def get_score(df_true, df_submission):\n",
    "    df_submission = df_submission.sort_values('pid')\n",
    "    df_true = df_true.sort_values('pid')\n",
    "    \n",
    "    task1 = np.mean([metrics.roc_auc_score(df_true[entry], df_submission[entry]) for entry in TESTS])\n",
    "    #task2 = metrics.roc_auc_score(df_true['LABEL_Sepsis'], df_submission['LABEL_Sepsis'])\n",
    "    #task3 = np.mean([0.5 + 0.5 * np.maximum(0, metrics.r2_score(df_true[entry], df_submission[entry])) for entry in VITALS])\n",
    "    #score = np.mean([task1, task2, task3])\n",
    "    return task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
